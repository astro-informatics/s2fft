{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Torch frontend guide__\n",
    "---\n",
    "\n",
    "[![colab image](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/astro-informatics/s2fft/blob/main/notebooks/torch_frontend.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Install s2fft and data if running on google colab.\n",
    "if IN_COLAB:\n",
    "    !pip install s2fft &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This minimal tutorial demonstrates how to use the torch frontend for `S2FFT` to compute spherical harmonic transforms. Though `S2FFT` is primarily designed for JAX, this torch functionality is fully unit tested (including gradients) and can be used straightforwardly as a learnable layer within existing models. As the torch functions wrap the JAX implementations we need to configure JAX to use 64-bit precision floating point types by default to ensure sufficient precision for the transforms - `S2FFT` will emit a warning if this has not been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import torch \n",
    "import numpy as np\n",
    "from s2fft.transforms.spherical import inverse, forward\n",
    "from s2fft.precompute_transforms.spherical import (\n",
    "    inverse as precompute_inverse, forward as precompute_forward\n",
    ")\n",
    "from s2fft.precompute_transforms.construct import spin_spherical_kernel_torch\n",
    "from s2fft.utils import signal_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets set up a mock problem by specifiying a bandlimit $L$ and generating some arbitrary harmonic coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 64 \n",
    "rng = np.random.default_rng(1234951510)\n",
    "flm = torch.from_numpy(signal_generator.generate_flm(rng, L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets calculate the signal on the sphere by applying the inverse spherical harmonic transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "f = inverse(flm, L, method=\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the corresponding spherical harmonic representation execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flm_check = forward(f, L, method=\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets check the error on the round trip is as expected for 64 bit machine precision floating point arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error = 2.8915048238993476e-14\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean absolute error = {np.nanmean(np.abs(flm_check - flm))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fully precompute transform we must also generate the precompute kernels which we store as a torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_kernel = spin_spherical_kernel_torch(L, forward=False) \n",
    "forward_kernel = spin_spherical_kernel_torch(L, forward=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass the kernels as additional arguments to the transform functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "precompute_f = precompute_inverse(flm, L, kernel=inverse_kernel, method=\"torch\")\n",
    "precompute_flm_check = precompute_forward(f, L, kernel=forward_kernel, method=\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we check the error on the round trip is as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error = 2.904741595325594e-14\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean absolute error = {np.nanmean(np.abs(precompute_flm_check - flm))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2fft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
