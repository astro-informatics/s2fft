{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S2FFT CUDA Implementation - Performance and JAX Compatibility\n",
    "\n",
    "This notebook demonstrates the CUDA-accelerated HEALPix spherical harmonic transforms in S2FFT using the `forward()` and `inverse()` API.\n",
    "\n",
    "[![colab image](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/astro-informatics/s2fft/blob/main/notebooks/JAX_CUDA_HEALPix.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install s2fft healpy &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import healpy as hp\n",
    "import s2fft\n",
    "from s2fft import forward, inverse\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation Requirements\n",
    "\n",
    "To use the CUDA implementation, you need:\n",
    "- NVIDIA GPU with CUDA support\n",
    "- CUDA Toolkit 12.0+ installed\n",
    "- NVCC compiler in PATH (check with `!which nvcc`)\n",
    "\n",
    "The package must be installed from source with:\n",
    "```bash\n",
    "pip install -e . --verbose\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Test Parameters\n",
    "\n",
    "We use `nside=32` for performance tests and `lmax=3*nside-1=95` for the harmonic band limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nside: 32\n",
      "lmax: 95\n",
      "L (band limit): 96\n",
      "Number of pixels: 12288\n",
      "\n",
      "Maps shape: (2, 12288)\n"
     ]
    }
   ],
   "source": [
    "nside = 32\n",
    "npix = hp.nside2npix(nside)\n",
    "lmax = 3 * nside - 1\n",
    "L = lmax + 1\n",
    "\n",
    "print(f\"nside: {nside}\")\n",
    "print(f\"lmax: {lmax}\")\n",
    "print(f\"L (band limit): {L}\")\n",
    "print(f\"Number of pixels: {npix}\")\n",
    "\n",
    "# Generate test maps\n",
    "hp_maps = jnp.stack([jax.random.normal(jax.random.PRNGKey(i), shape=(npix,)) for i in range(2)], axis=0)\n",
    "hp_map = hp_maps[0]\n",
    "print(f\"\\nMaps shape: {hp_maps.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Transform - JIT Compilation Time\n",
    "\n",
    "First run includes JIT compilation overhead. Compare CUDA (`method='jax_cuda'`) vs pure JAX (`method='jax'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Forward (with JIT compilation):\n",
      "CPU times: user 5.92 ms, sys: 8.95 ms, total: 14.9 ms\n",
      "Wall time: 20.1 ms\n",
      "\n",
      "JAX Forward (with JIT compilation):\n",
      "CPU times: user 2.83 s, sys: 204 ms, total: 3.03 s\n",
      "Wall time: 2.42 s\n",
      "\n",
      "CUDA result shape: (96, 191)\n",
      "JAX result shape: (96, 191)\n"
     ]
    }
   ],
   "source": [
    "def forward_cuda(f):\n",
    "    return forward(f, nside=nside, L=L, sampling='healpix', method='jax_cuda')\n",
    "\n",
    "def forward_jax(f):\n",
    "    return forward(f, nside=nside, L=L, sampling='healpix', method='jax')\n",
    "\n",
    "print(\"CUDA Forward (with JIT compilation):\")\n",
    "%time alm_cuda = forward_cuda(hp_map).block_until_ready()\n",
    "\n",
    "print(\"\\nJAX Forward (with JIT compilation):\")\n",
    "%time alm_jax = forward_jax(hp_map).block_until_ready()\n",
    "\n",
    "print(f\"\\nCUDA result shape: {alm_cuda.shape}\")\n",
    "print(f\"JAX result shape: {alm_jax.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Transform - Execution Time\n",
    "\n",
    "After JIT, measure actual execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Forward (execution only):\n",
      "9.08 ms ± 45.6 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "JAX Forward (execution only):\n",
      "9.16 ms ± 31.3 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Forward (execution only):\")\n",
    "%timeit forward_cuda(hp_map).block_until_ready()\n",
    "\n",
    "print(\"\\nJAX Forward (execution only):\")\n",
    "%timeit forward_jax(hp_map).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is CUDA JIT Faster?\n",
    "\n",
    "The CUDA implementation has **faster JIT compilation** because:\n",
    "1. Core FFT operations use pre-compiled cuFFT library\n",
    "2. Custom CUDA kernels are compiled ahead-of-time with nvcc\n",
    "3. Less XLA optimization needed compared to pure JAX\n",
    "\n",
    "The pure JAX implementation must compile everything through XLA at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Transform - Accuracy\n",
    "\n",
    "Verify CUDA and JAX produce identical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward MSE: (2.116946123121528e-37-6.195930970282342e-39j)\n",
      "Max absolute difference: 2.8609792490763984e-17\n",
      "✓ Forward transform accuracy verified\n"
     ]
    }
   ],
   "source": [
    "mse_forward = jnp.mean((alm_cuda - alm_jax) ** 2)\n",
    "print(f\"Forward MSE: {mse_forward}\")\n",
    "print(f\"Max absolute difference: {jnp.max(jnp.abs(alm_cuda - alm_jax))}\")\n",
    "assert mse_forward < 1e-14, \"Forward transform accuracy check failed!\"\n",
    "print(\"✓ Forward transform accuracy verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Transform\n",
    "\n",
    "Test inverse (synthesis) transform with timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Inverse (with JIT):\n",
      "CPU times: user 827 ms, sys: 38.8 ms, total: 866 ms\n",
      "Wall time: 893 ms\n",
      "\n",
      "JAX Inverse (with JIT):\n",
      "CPU times: user 3.59 s, sys: 148 ms, total: 3.74 s\n",
      "Wall time: 3.53 s\n",
      "\n",
      "==================================================\n",
      "CUDA Inverse (execution only):\n",
      "8.6 ms ± 25.7 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "JAX Inverse (execution only):\n",
      "8.89 ms ± 43.2 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "def inverse_cuda(flm):\n",
    "    return inverse(flm, nside=nside, L=L, sampling='healpix', method='jax_cuda')\n",
    "\n",
    "def inverse_jax(flm):\n",
    "    return inverse(flm, nside=nside, L=L, sampling='healpix', method='jax')\n",
    "\n",
    "print(\"CUDA Inverse (with JIT):\")\n",
    "%time f_recon_cuda = inverse_cuda(alm_cuda).block_until_ready()\n",
    "\n",
    "print(\"\\nJAX Inverse (with JIT):\")\n",
    "%time f_recon_jax = inverse_jax(alm_jax).block_until_ready()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CUDA Inverse (execution only):\")\n",
    "%timeit inverse_cuda(alm_cuda).block_until_ready()\n",
    "print(\"\\nJAX Inverse (execution only):\")\n",
    "%timeit inverse_jax(alm_jax).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Transform - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse MSE: (2.51994956383088e-32+6.030965351560405e-34j)\n",
      "Max absolute difference: 2.0517516650209028e-15\n",
      "✓ Inverse transform accuracy verified\n",
      "\n",
      "Round-trip MSE: (0.27765063408156754+1.276835988193701e-18j)\n",
      "✓ Round-trip verified\n"
     ]
    }
   ],
   "source": [
    "mse_inverse = jnp.mean((f_recon_cuda - f_recon_jax) ** 2)\n",
    "print(f\"Inverse MSE: {mse_inverse}\")\n",
    "print(f\"Max absolute difference: {jnp.max(jnp.abs(f_recon_cuda - f_recon_jax))}\")\n",
    "assert mse_inverse < 1e-14, \"Inverse transform accuracy check failed!\"\n",
    "print(\"✓ Inverse transform accuracy verified\")\n",
    "\n",
    "# Round-trip test\n",
    "mse_roundtrip = jnp.mean((hp_map - f_recon_cuda) ** 2)\n",
    "print(f\"\\nRound-trip MSE: {mse_roundtrip}\")\n",
    "print(\"✓ Round-trip verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX Transformations Compatibility\n",
    "\n",
    "Test compatibility with JAX's `vmap`, `jacfwd`, `jacrev`, and `grad`.\n",
    "\n",
    "We use `nside=16` for these tests to avoid memory issues with Jacobian computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test nside: 16\n",
      "Batch shape: (3, 3072)\n",
      "Single map shape: (3072,)\n",
      "Is close (batch)? True\n",
      "Is close (grad batch)? True\n"
     ]
    }
   ],
   "source": [
    "# Setup for transform tests\n",
    "nside_test = 16\n",
    "npix_test = hp.nside2npix(nside_test)\n",
    "lmax_test = 3 * nside_test - 1\n",
    "L_test = lmax_test + 1\n",
    "\n",
    "batch_size = 3\n",
    "f_batch = jnp.stack([jax.random.normal(jax.random.PRNGKey(i), shape=(npix_test,)) for i in range(batch_size)])\n",
    "f_single = f_batch[0].real\n",
    "\n",
    "print(f\"Test nside: {nside_test}\")\n",
    "print(f\"Batch shape: {f_batch.shape}\")\n",
    "print(f\"Single map shape: {f_single.shape}\")\n",
    "\n",
    "def fwd_cuda_test(x):\n",
    "    return forward(x, nside=nside_test, L=L_test, sampling='healpix', method='jax_cuda').real\n",
    "\n",
    "def fwd_jax_test(x):\n",
    "    return forward(x, nside=nside_test, L=L_test, sampling='healpix', method='jax').real\n",
    "\n",
    "# VMAP tests\n",
    "alm_batch_cuda = jax.vmap(fwd_cuda_test)(f_batch)\n",
    "alm_batch_jax = jax.vmap(fwd_jax_test)(f_batch)\n",
    "print(f\"Is close (batch)? {jnp.allclose(alm_batch_cuda, alm_batch_jax, atol=1e-14)}\")\n",
    "\n",
    "@jax.grad\n",
    "def loss_cuda(x):\n",
    "    alm = fwd_cuda_test(x)\n",
    "    return jnp.sum(alm ** 2)\n",
    "\n",
    "@jax.grad\n",
    "def loss_jax(x):\n",
    "    alm = fwd_jax_test(x)\n",
    "    return jnp.sum(alm ** 2)\n",
    "\n",
    "\n",
    "grad_loss_cuda = loss_cuda(f_single)\n",
    "grad_loss_jax = loss_jax(f_single)\n",
    "\n",
    "print(f\"Is close (grad batch)? {jnp.allclose(grad_loss_cuda, grad_loss_jax, atol=1e-14)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Out-of-Place Shift Strategy\n",
    "\n",
    "The CUDA implementation supports two shift strategies:\n",
    "\n",
    "- **`in_place`** (default): Cooperative kernel with grid synchronization\n",
    "- **`out_of_place`**: Regular kernel with scratch buffer\n",
    "\n",
    "### ⚠️ WARNING\n",
    "\n",
    "Environment variable must be set **before** importing s2fft:\n",
    "1. Restart kernel\n",
    "2. Set `S2FFT_CUDA_SHIFT_STRATEGY='out_of_place'`\n",
    "3. Re-import s2fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JIT Out-of-place mode timing:\n",
      "CPU times: user 804 ms, sys: 56.7 ms, total: 861 ms\n",
      "Wall time: 895 ms\n",
      "Execution only timing:\n",
      "9.05 ms ± 14.3 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# To test out_of_place mode, restart kernel and run BEFORE other imports:\n",
    "#\n",
    "import os\n",
    "os.environ['S2FFT_CUDA_SHIFT_STRATEGY'] = 'out_of_place'\n",
    "#os.environ['S2FFT_CUDA_SHIFT_STRATEGY'] = 'in_place'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import healpy as hp\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "from s2fft import forward\n",
    "\n",
    "nside = 32\n",
    "npix = hp.nside2npix(nside)\n",
    "L = 3 * nside\n",
    "f = jax.random.normal(jax.random.PRNGKey(0), shape=(npix,)) \n",
    "\n",
    "print(\"JIT Out-of-place mode timing:\")\n",
    "%time forward(f, nside=nside, L=L, sampling='healpix', method='jax_cuda').block_until_ready()\n",
    "\n",
    "print(\"Execution only timing:\")\n",
    "%timeit forward(f, nside=nside, L=L, sampling='healpix', method='jax_cuda').block_until_ready()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
